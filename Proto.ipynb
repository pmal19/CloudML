{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import transforms, utils\n",
    "from itertools import *\n",
    "\n",
    "from dataparser import *\n",
    "from batcher import *\n",
    "from readEmbeddings import *\n",
    "from datasets import *\n",
    "from models import *\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class sstDataset(Dataset):\n",
    "    def __init__(self, sstPath, glovePath, transform = None):\n",
    "        self.data = load_sst_data(sstPath)\n",
    "        self.paddingElement = ['<s>']\n",
    "        self.maxSentenceLength = self.maxlength(self.data)\n",
    "        self.vocab = glove2dict(glovePath)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s = self.pad(self.data[index]['sentence_1'].split())\n",
    "        s = self.embed(s)\n",
    "        label = int(self.data[index]['label'])\n",
    "        return (s), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def maxlength(self, data):\n",
    "        maxSentenceLength = max([len(d['sentence_1'].split()) for d in data])\n",
    "#         maxLength = max(maxLength,maxSentenceLength)\n",
    "        return maxSentenceLength\n",
    "\n",
    "    def pad(self, sentence):\n",
    "        return sentence + (51-len(sentence))*self.paddingElement\n",
    "\n",
    "    def embed(self, sentence):\n",
    "        vector = []\n",
    "        for word in sentence:\n",
    "            if str(word) in self.vocab:\n",
    "                vector = np.concatenate((vector, self.vocab[str(word)]), axis=0)\n",
    "            else:\n",
    "                vector = np.concatenate((vector, [0]*len(self.vocab['a'])), axis=0)\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Partition(Dataset):\n",
    "    def __init__(self, data, index):\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_idx = self.index[index]\n",
    "        return self.data[data_idx]\n",
    "\n",
    "\n",
    "class DataPartitioner(Dataset):\n",
    "    def __init__(self, data, sizes, seed=1234):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        data_len = len(data)\n",
    "        indexes = [x for x in range(0, data_len)]\n",
    "        rng.shuffle(indexes)\n",
    "\n",
    "        for frac in sizes:\n",
    "            part_len = int(frac * data_len)\n",
    "            self.partitions.append(indexes[0:part_len])\n",
    "            indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "\n",
    "def partition_dataset(sstPath, glovePath, batchSize, transformations=None):\n",
    "    dataset = sstDataset(sstPath, glovePath, transformations)\n",
    "    size = dist.get_world_size()\n",
    "    bsz = batchSize\n",
    "    partition_sizes = [1.0 / size for _ in range(size)]\n",
    "    partition = DataPartitioner(dataset, partition_sizes)\n",
    "    partition = partition.use(dist.get_rank())\n",
    "    train_set = DataLoader(partition, batch_size=bsz, shuffle=True, num_workers=1)\n",
    "    return train_set, bsz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassLSTM(nn.Module):\n",
    "    \"\"\"docstring for ClassLSTM\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch, bias = True, batch_first = False, dropout = 0, bidirectional = False):\n",
    "        super(ClassLSTM, self).__init__()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bias = True, batch_first = False, dropout = dropout, bidirectional = bidirectional)\n",
    "        self.h0 = Variable(torch.randn(num_layers * self.num_directions, batch, hidden_size))\n",
    "        self.c0 = Variable(torch.randn(num_layers * self.num_directions, batch, hidden_size))\n",
    "    def forward(self, s1):\n",
    "        output, hn = self.lstm(s1, (self.h0, self.c0))\n",
    "        return output\n",
    "\n",
    "class sstNet(nn.Module):\n",
    "    \"\"\"docstring for sstNet\"\"\"\n",
    "    def __init__(self, inp_dim, model_dim, num_layers, reverse, bidirectional, dropout, mlp_input_dim, mlp_dim, num_classes, num_mlp_layers, mlp_ln, classifier_dropout_rate, training, batchSize):\n",
    "        super(sstNet, self).__init__()\n",
    "        self.encoderSst = ClassLSTM(inp_dim, model_dim, num_layers, batchSize, bidirectional = bidirectional, dropout = dropout)\n",
    "        self.classifierSst = MLP(mlp_input_dim, mlp_dim, num_classes, num_mlp_layers, mlp_ln, classifier_dropout_rate, training)\n",
    "\n",
    "    def forward(self, s1):\n",
    "        oE = self.encoderSst(s1)\n",
    "        features = oE[-1]\n",
    "        output = F.log_softmax(self.classifierSst(features))\n",
    "        return output\n",
    "\n",
    "    def encode(self, s):\n",
    "        emb = self.encoderSst(s)\n",
    "        return emb\n",
    "    \n",
    "class BiLSTMSentiment(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, use_gpu, batch_size, dropout=0.5):\n",
    "        super(BiLSTMSentiment, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim*2, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.use_gpu:\n",
    "            return (Variable(torch.zeros(2, self.batch_size, self.hidden_dim).cuda()),\n",
    "                    Variable(torch.zeros(2, self.batch_size, self.hidden_dim).cuda()))\n",
    "        else:\n",
    "            return (Variable(torch.zeros(2, self.batch_size, self.hidden_dim)),\n",
    "                    Variable(torch.zeros(2, self.batch_size, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        x = sentence.view(len(sentence), self.batch_size, -1)\n",
    "        lstm_out, _ = self.lstm(x, self.hidden)\n",
    "        y = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainEpoch(epoch, break_val, trainLoader, model, optimizer, criterion, inp_dim, batchSize, use_cuda, devLoader, devbatchSize):\n",
    "    print(\"Epoch start - \",epoch)\n",
    "    for batch_idx, (data, target) in enumerate(trainLoader):\n",
    "        s1 = data.float()\n",
    "        batch, _ = s1.shape\n",
    "        if batchSize != batch:\n",
    "            break\n",
    "        s1 = s1.transpose(0,1).contiguous().view(-1,inp_dim,batch).transpose(1,2)\n",
    "        if(use_cuda):\n",
    "            s1, target = Variable(s1.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            s1, target = Variable(s1), Variable(target)\n",
    "        \n",
    "#         output = model(s1)\n",
    "        \n",
    "        # pdb.set_trace()\n",
    "        model.zero_grad()\n",
    "        output = model(s1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "#         loss.backward(retain_graph=True)\n",
    "#         optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "#         print(batch_idx,loss)\n",
    "        if batch_idx == break_val:\n",
    "            return\n",
    "        if batch_idx % 100 == 0:\n",
    "            dev_loss = 0\n",
    "            n_correct = 0\n",
    "            n_total = 0\n",
    "            for idx, (dev_data, dev_target) in enumerate(devLoader):\n",
    "                sd = dev_data.float()\n",
    "                # pdb.set_trace()\n",
    "                devbatchSize, _ = sd.shape\n",
    "                if batchSize != devbatchSize:\n",
    "                    break\n",
    "#                 print(\"before dev\",sd.shape)\n",
    "                sd = sd.transpose(0,1).contiguous().view(-1,inp_dim,devbatchSize).transpose(1,2)\n",
    "#                 print(\"after dev\",sd.shape)\n",
    "                if(use_cuda):\n",
    "                    sd, dev_target = Variable(sd.cuda()), Variable(dev_target.cuda())\n",
    "                else:\n",
    "                    sd, dev_target = Variable(sd), Variable(dev_target)\n",
    "                dev_output = model(sd)\n",
    "                dev_loss += criterion(dev_output, dev_target)\n",
    "                n_correct += (torch.max(dev_output, 1)[1].view(dev_target.size()) == dev_target).sum()\n",
    "                n_total += devbatchSize\n",
    "                # break\n",
    "            dev_acc = (100. * n_correct.data[0])/n_total\n",
    "\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tDev Loss: {:.6f}\\tDev Acc: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(trainLoader.dataset),\n",
    "                100. * batch_idx / len(trainLoader), loss.data[0], dev_loss.data[0], dev_acc))\n",
    "#             save(model, optimizer, loss, 'sstTrained.pth', dev_loss, dev_acc)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(numEpochs, trainLoader, model, optimizer, criterion, inp_dim, batchSize, use_cuda, devLoader, devbatchSize):\n",
    "    for epoch in range(numEpochs):\n",
    "        loss = trainEpoch(epoch,20000000,trainLoader,model,optimizer,criterion,inp_dim,batchSize, use_cuda, devLoader, devbatchSize)\n",
    "#         dev_loss = 0\n",
    "#         n_correct = 0\n",
    "#         n_total = 0\n",
    "#         for idx, (dev_data, dev_target) in enumerate(devLoader):\n",
    "#             sd = dev_data.float()\n",
    "#             # pdb.set_trace()\n",
    "#             devbatchSize, _ = sd.shape\n",
    "#             sd = sd.transpose(0,1).contiguous().view(-1,inp_dim,devbatchSize).transpose(1,2)\n",
    "#             if(use_cuda):\n",
    "#                 sd, dev_target = Variable(sd.cuda()), Variable(dev_target.cuda())\n",
    "#             else:\n",
    "#                 sd, dev_target = Variable(sd), Variable(dev_target)\n",
    "#             dev_output = model(sd)\n",
    "#             dev_loss += criterion(dev_output, dev_target)\n",
    "#             n_correct += (torch.max(dev_output, 1)[1].view(dev_target.size()) == dev_target).sum()\n",
    "#             n_total += devbatchSize\n",
    "#         dev_acc = (100. * n_correct.data[0])/n_total\n",
    "#         print('Epoch: {} - Dev Accuracy: {}'.format(epoch, dev_acc))\n",
    "# #         save(model, optimizer, loss, 'sstTrainedEpoch.pth', dev_loss, dev_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST eval mode: Preserving only top node label.\n",
      "SST eval mode: Preserving only top node label.\n",
      "('Time taken - ', 32.92652916908264)\n"
     ]
    }
   ],
   "source": [
    "sstPathTrain = \"../Data/SST/trees/train.txt\"\n",
    "sstPathDev = \"../Data/SST/trees/dev.txt\"\n",
    "glovePath = '../Data/glove.6B/glove.6B.100d.txt'\n",
    "\n",
    "batchSize = 16\n",
    "learningRate = 0.0001\n",
    "momentum = 0.9\n",
    "numWorkers = 0\n",
    "\n",
    "numEpochs = 1\n",
    "\n",
    "inp_dim = 100\n",
    "model_dim = 100\n",
    "num_layers = 2\n",
    "reverse = False\n",
    "bidirectional = True\n",
    "dropout = 0.4\n",
    "\n",
    "mlp_input_dim = 200\n",
    "mlp_dim = 100\n",
    "num_classes = 5\n",
    "num_mlp_layers = 5\n",
    "mlp_ln = True\n",
    "classifier_dropout_rate = 0.4\n",
    "\n",
    "training = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "if(use_cuda):\n",
    "    the_gpu.gpu = 0\n",
    "\n",
    "t1 = time.time()\n",
    "trainingDataset = sstDataset(sstPathTrain, glovePath)\n",
    "devDataset = sstDataset(sstPathDev, glovePath)\n",
    "print('Time taken - ',time.time()-t1)\n",
    "devbatchSize = batchSize\n",
    "\n",
    "trainLoader = DataLoader(trainingDataset, batchSize, shuffle=False, num_workers = numWorkers)\n",
    "devLoader = DataLoader(devDataset, devbatchSize, shuffle=False, num_workers = numWorkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch start - ', 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pramitmallick/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/pramitmallick/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:51: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/pramitmallick/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:55: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/8544 (0%)]\tLoss: 1.605799\tDev Loss: 108.582504\tDev Acc: 25.000000\n",
      "Train Epoch: 0 [1600/8544 (19%)]\tLoss: 1.446144\tDev Loss: 108.963860\tDev Acc: 25.000000\n",
      "Train Epoch: 0 [3200/8544 (37%)]\tLoss: 1.200227\tDev Loss: 146.160980\tDev Acc: 25.000000\n",
      "Train Epoch: 0 [4800/8544 (56%)]\tLoss: 1.636848\tDev Loss: 108.261330\tDev Acc: 25.000000\n",
      "Train Epoch: 0 [6400/8544 (75%)]\tLoss: 1.575116\tDev Loss: 107.864517\tDev Acc: 25.000000\n",
      "Train Epoch: 0 [8000/8544 (94%)]\tLoss: 1.472182\tDev Loss: 108.349686\tDev Acc: 25.000000\n"
     ]
    }
   ],
   "source": [
    "# model = sstNet(inp_dim, model_dim, num_layers, reverse, bidirectional, dropout, mlp_input_dim, mlp_dim, num_classes, num_mlp_layers, mlp_ln, classifier_dropout_rate, training, batchSize)\n",
    "model = BiLSTMSentiment(100, 100, 100, 5, False, batchSize) \n",
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "if(use_cuda):\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "criterion=nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learningRate, weight_decay = 1e-5)\n",
    "\n",
    "train(numEpochs, trainLoader, model, optimizer, criterion, inp_dim, batchSize, use_cuda, devLoader, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = \"../Data/SST/sentiment_dataset/data/stsa.fine.dev\"\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "# len(content)\n",
    "# print(content)\n",
    "devset = []\n",
    "for line in content:\n",
    "    cls = int(line[0])\n",
    "    if cls < 2:\n",
    "        cls = 0\n",
    "    elif cls == 2:\n",
    "        cls = 1\n",
    "    else:\n",
    "        cls = 2\n",
    "    line = line[2:].split()\n",
    "    devset.append([cls,line])\n",
    "#     print([cls,line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
