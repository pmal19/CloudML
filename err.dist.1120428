/scratch/pm2758/distTraining/Code/mp_dist_trial.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  log_probs = F.log_softmax(y)
/scratch/pm2758/anaconda3/envs/pytorch_v1/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/scratch/pm2758/distTraining/Code/mp_dist_trial.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  log_probs = F.log_softmax(y)
/scratch/pm2758/anaconda3/envs/pytorch_v1/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/scratch/pm2758/distTraining/Code/mp_dist_trial.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  log_probs = F.log_softmax(y)
/scratch/pm2758/anaconda3/envs/pytorch_v1/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/scratch/pm2758/distTraining/Code/mp_dist_trial.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  log_probs = F.log_softmax(y)
/scratch/pm2758/anaconda3/envs/pytorch_v1/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Traceback (most recent call last):
  File "mp_dist_trial.py", line 303, in <module>
    run_demo(setupAndCall, 4)
  File "mp_dist_trial.py", line 299, in run_demo
    join=True)
  File "/scratch/pm2758/anaconda3/envs/pytorch_v1/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 167, in spawn
    while not spawn_context.join():
  File "/scratch/pm2758/anaconda3/envs/pytorch_v1/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 114, in join
    raise Exception(msg)
Exception: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/scratch/pm2758/anaconda3/envs/pytorch_v1/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/scratch/pm2758/distTraining/Code/mp_dist_trial.py", line 278, in setupAndCall
    main(rank, world_size)
  File "/scratch/pm2758/distTraining/Code/mp_dist_trial.py", line 250, in main
    weighted_loss, numberOfSamples, average_time = run(rank, wsize, model, optimizer, criterion, epochs, trainLoader, bszTrain, devLoader, use_cuda, batchSize, batchSize, 100)
  File "/scratch/pm2758/distTraining/Code/mp_dist_trial.py", line 173, in run
    average_gradients(model)
  File "/scratch/pm2758/distTraining/Code/mp_dist_trial.py", line 142, in average_gradients
    dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=0)
  File "/scratch/pm2758/anaconda3/envs/pytorch_v1/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 902, in all_reduce
    work = group.allreduce([tensor], opts)
AttributeError: 'int' object has no attribute 'allreduce'

